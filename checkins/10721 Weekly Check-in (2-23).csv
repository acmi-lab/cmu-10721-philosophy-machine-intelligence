Timestamp,Name (First + Last) ,Email ID,Question 1,Question 2,Question 3
2/22/2023 22:45:22,Conny Knieling,"cok22@pitt.edu, cknielin@andrew.cmu.edu","Vapnik: A more general question, but I wonder what would have happened (to Ml and to Statistics) if Statistical Learning had been more accepted and then incorporated by Statistics","Spelda etal: Is inductive risk avoidable (I'd strongly argue no)? and if not, what is the best way to address it? I am not fully convinced by their solution. I also like the thought behind the contract-idea but disagree with the contract-metaphor.","Sterkenburg etal: They argue that the NFL theorems might not apply to learning algorithms. Granting that, could there not be similar NFL theorems for those? And can statistical learning avoid bias, NFL theorem-equivalent or not. "
2/23/2023 1:38:18,Emmy Liu,mengyan3,"1. I'm a little confused about what transduction is and why it isn't a form of induction. It seems like it is described as just modifying the decision rule based on data, but isn't that always happening, for instance when a model is trained online? Wouldn't this still just be induction but applied over some more data?","2. Does learning ""robust regularities"" (that hold at least in our environment on earth)mean that we should have more confidence in ML models in the case of distribution shift? In other words, can we solve this problem for practical purposes with careful use of IRM? ","3. The connection of generalization to the uniformity of ""possible worlds"" is interesting. The article seems to imply this isn't really possible in principle (measuring the distribution of possible learning situations), but I wonder if something could be said about it in specific cases. "
2/23/2023 9:49:50,Saujas Vaduguru,svadugur@andrew.cmu.edu,"Harman and Kulkarni distinguish the notion of inference from ""identifying what follows what"" as having to do with a change in belief, while the notion of induction in John Norton's work does not relate to belief/psychology. What would be different about Harman and Kulkarni's ideas if we were to abandon the psychological point of view?","Vapnik writes that ""Since one was able to find the function that separates the training data well, in the set of functions that is easy to falsify, these data are very special and the function which one chooses reflects the intrinsic properties of these data."" How does this fit with the instrumentalist view that seems agnostic to ""intrinsic properties"" of data?",Do we need to rethink the perspective of parsimony/Occam's razor in the era of ever growing model size?
2/23/2023 9:58:36,Lindia Tjuatja,lindiat,"I'm not quite sure what Spelda and Stritecky mean by the Human-Machine Nexus, can we get a definition for that?",What justifies the use of one prior over another if they give the same performance?,"What is the role of simplicity in ML other than for human understanding? If we assume the universe is inherently complex, then why is this requirement considered necessary for some?"
2/23/2023 10:53:51,Ji Min Mun,jmun,Does the no free lunch theorem still apply in deep learning?,How is philosophy of induction applied in statistical learning and deep learning?,Where does this week's readings leave us on the model vs data debate?
2/23/2023 11:14:07,Patrick Fernandes,pfernand,Do the two cultures mentioned by Breiman reflect the same positions of realism vs instrumentalism in the same way that Vapnik’s classical discriminant analysis vs statistical learning theory do?,"Are we limited to an “intrumentalism” position with modern deep learning, where we are limited to making statements about errors and predictions? Or can techniques like mechanistic interpretability allows to make statements about the function space and closer to “realist” position? ","The No-Free-Lunch reading states that learning algorithms should be evaluated with respect to a model (class), since by the NFL theorems, no general data-driven-only algorithm is better than any other one. How do we reconcile this statement with the fact Empirical Risk Minimization by Gradient Descent works so well for many different (neural network) models? What kinds of inductive biases are these architectures implying to allow ERM to surpass other learning approaches? "